input {
    beats {
        port => "5043"
    }
}
filter {
    # For now, we have reactomews and everything else is assumed to come from reactome.org
    if [source] =~ ".*reactomews.*" {
        mutate {
            add_field => {"reactome_server" => "reactomews"}
        }
    }
    else {
        mutate {
            add_field => {"reactome_server" => "reactome.org"}
        }
    }
    # Fingerprint to avoid duplicate messages
    fingerprint {
        source => "message"
        target => "[@metadata][fingerprint]"
        method => "MURMUR3"
    }
    # decode encoded values in the message.
    urldecode {
        field => "message"
    }
    # Parse Apache log message.
    if [reactome_server] == "reactome.org" {
        grok {
          match => { "message" => ["%{COMBINEDAPACHELOG}"] }
          tag_on_failure => ["grok_not_COMBINEDAPACHELOG"]
        }
    }
    else if [reactome_server] == "reactomews" {
        grok {
          match => { "message" => ["%{COMMONAPACHELOG}"] }
          tag_on_failure => ["grok_not_COMMONAPACHELOG"]
        }
    }
    # extract date from logfile and set it as a proper "date" field, so it can be indexed and used instead the default timestamp.
#    mutate {
#        copy => { "@timestamp" => "ingestion_timestamp" }
#    }
    date {
      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@apache_timestamp"
    }
#    date {
#      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
#    }

    # Now, extract some specific date fields, so that they can be used in reports.
    ruby {
        code => 'event.set("dayOfWeek", Time.parse(event.get("@apache_timestamp").to_s).strftime("%w - %A"))'
    }
    ruby {
        code => 'event.set("weekOfYear", Time.parse(event.get("@apache_timestamp").to_s).strftime("%U"))'
    }
    grok {
      match => { "message" => ["^.*%{MONTHDAY:dayOfMonth}\/%{MONTH:monthName}\/%{YEAR:year}:(?!<[0-9])%{HOUR:hourOfDay}:%{MINUTE}(?::%{SECOND})(?![0-9]) %{INT:utcOffset}.*$"] }
    }
    # Extract URL parameters from the "request" field.
    grok {
        match => [ "request", "%{URIPARAM:url}" ]
        tag_on_failure => ["grok_no_URIPARAM"]
    }
    # Process the referrer URL
    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        match => ["referrer", "%{REFERRER_URL}"]
        tag_on_failure => ["grok_referrer_parse_err"]
    }
    # Extract Reactome stable identifier information from the "request" field
    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        match => [ "request" , "%{STABLE_IDENTIFIER:reactome_stable_identifier}"]
        tag_on_failure => ["grok_no_reactome_stable_identifier"]
    }
    # Extract the name of the Reactome Application that was requested (could return null)
    grok {
        match => ["request", "%{REACTOME_APPLICATION:reactome_application}"]
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        tag_on_failure => ["grok_no_reactome_application"]
    }
    # Add a field to indicate a request came from an internal IP address (relative to the server)
    # These requests are probably coming from other Reactome servers, OR from OICR staff.
    cidr {
        add_field => {"internal_IP" => true }
        address => [ "%{clientip}" ]
        network => [ "10.0.0.0/8", "172.16.0.0/12", "192.168.0.0/16", "fc00::/7", "::1", "::", "127.0.0.1", "0.0.0.0" ]
    }
    if [reactome_application] =~ /(?i)\/?reactomerestfulapi/ {
        grok {
            match => ["request", "%{RESTFUL_API_PATH}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_RESTfulAPI_path"]
        }
    }
    if [reactome_application] =~ /(?i)\/?contentservice/ {
        grok {
            match => ["request", "%{CONTENT_SERVICE_PATH}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_ContentService_path"]
        }
    }
    if [reactome_application] =~ /(?i)\/?analysisservice/ {
        grok {
            match => ["request", "%{ANALYSIS_SERVICE_PATH}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_AnalysisService_path"]
        }
    }
    if [reactome_application] =~ /(?i)\/?cgi-bin/ {
        grok {
            match => ["request", "%{CGI_SCRIPT_NAME}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_CGIScipt"]
        }
    }
    # Get the download file name, and also clean up the reactome_application field
    # because the filename is probably in it.
    if [reactome_application] =~ /(?i)\/?download\/current\//
        and [reactome_application] !~ /(?i)\/?download\/current\/(fireworks|diagram)/ {

        mutate {
            replace => { "reactome_application" => "/download/current/" }
        }
        grok {
            match => ["request", "%{DOWNLOAD_FILE}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_DownloadFileName"]
        }
    }
    if [reactome_application] =~ /(?i).*caBigR3WebApp.*/ {
        grok {
            match => ["request", "%{CABIGR3WEBAPP_PATH}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_CABIGR3WEBAPP_PATH"]
        }
    }
    if [reactome_application]
    {
        mutate
        {
            lowercase => [reactome_application]
        }
    }
    # transform the bytes field to an integer so Kibana can do SUM on it. Also clean up the URL a bit.
    # Also convert certain date fields to numbers, so they can be used in histograms.
    mutate {
        convert => {"bytes" => "integer"}
        convert => {"dayOfMonth" => "integer"}
        convert => {"hourOfDay" => "integer"}
        convert => {"year" => "integer"}
        convert => {"weekOfYear" => "integer"}
        gsub => [
                "url","&amp;","&",
                "url","&amp%3B","&",
                "url","amp%3B","&",
                "url","%3B",";",
                "url","&amp","&",
                "url","%230","",
                "url","load%5B%5D","load",
                "url",";amp;id","id",
                "url","VALUE#0","VALUE",
                "url","OPERATOR#0","OPERATOR",
                "url","ATTRIBUTE#0","ATTRIBUTE"
                ]
    }

    # process the useragent information
    useragent {
      source => "agent"
      target => "useragent"
    }
    # Add a field to make it easier to find messages from bots.
    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        match => ["useragent", "%{BOT}"]
        tag_on_failure => ["grok_BOT_parse_fail"]
        add_field => {"probably_a_bot" => true }
    }

    # Extract key-value pairs from URL string and index the keys, but only if they are REACTOME keys.
    # Ignore when the referrer is a search engine.
    # Be aware that sometimes the referrer might look like "http://www.google.com/url=http://reactome.org/blah-blahblah"
    # Also ignore when referrer is WordPress admin or Joomla admin (we don't need to index those keys for log analysis)
    # Also: do not perform KV processing when request is from a bot - too much potential junk in requests from bots.
    # Also: Don't bother with KV processing if the request is just for favicon or something trivial like that.
    # How about this: ONLY bother with KV processing for known Reactome Applications? Should simplify things by a lot!
    if ![probably_a_bot] and [reactome_application] and ([request] !~ ".*<script.*" and [request] !~ ".*script>.*")
        and ([request] !~ "^\/favicon.ico$"
            and [request] !~ "^\/icons\/search\/Pathway\.gif$"
            and [request] !~ "^\/images\/spinner\.gif$"
            and [request] !~ "^\/stylesheet\.css$"
            and [request] !~ ".*getTags\?tagName=http.*")
        and ([referrer] =~ "http:\/\/reactome.org" or [referrer] =~ "https:\/\/reactome.org"
            or [referrer] =~ "http:\/\/www.reactome.org" or [referrer] =~ "https:\/\/www.reactome.org"
            or [referrer] =~ "http:\/\/reactomeprd1.oicr.on.ca" or [referrer] =~ "https:\/\/reactomeprd1.oicr.on.ca"
            or [referrer] !~ ".*")
        and ([referrer] !~ "google" and [referrer] !~ "yahoo" and [referrer] !~ "baidu" and [referrer] !~ "bing" and [referrer] !~ "yandex"
            and [referrer] !~ "ask." and [referrer] !~ "aol." and [referrer] !~ "duckduckgo" and [referrer] !~ "wolframalpha"
            and [referrer] !~ "wp-admin" and [referrer] !~ "wordpress" and [referrer] !~ "\/administrator\/index.php"
            and [request] !~ "wp-admin" and [request] !~ "wordpress" and [request] !~ "\/administrator\/index.php")
    {
        kv {
            source => "url"
            field_split => "&"
            trim_key => "\?"
            transform_key => "lowercase"
        }
    }

    prune {
        # a list of keys we want to prune. This list may grow over time.
        blacklist_names => [".*(z|Z)(o|O|0){2}(m|M).*","xml_sitemap",
            "toolbar", "treeView", "_fp[0-9]+","^(\'(A|a))$","_sm_au_",".*<script>.*",
            "^width$", "^height$","_wpnonce","<meta%20http-equiv","document_srl","conversation(C|c)ontext",
            "^do$", "^mod$", "^op$", "pwb_redirect", "submit(search)?", "^task$", "^view(image)?$",
            "^keyword$", "^dopost$", "^fmdo$", "^array$", "c@@\.12c\.c@\.\(c\)\(c\(", "inslockfile", "install_demo_name",
            "^s_lang$", "isappinstalled", "^coppa$", "^agreed$", "^fid$", "mod(e)?", "visitdsttime", "preview_nonce",
            "^usg$", "^ved$", "^sa$", "tbpm", "s,ql","sbmlRetrieval\/allForms\?speciesId", "redlink",
            "nsukey","^ll$", "maxadcountsperinterval", "method:#_memberaccess%\.url",
            "infloat", "inajax", "ajaxmenu", "handlekey", "gf_page", "^fid$", "entryac",
            "cssreloader2345", "cperpage", "classic", "chvzaa", "bustype", "beacon", "adcountintervalhours",
            "^1$", "^4$", "^2$", "stichwort", "wx_fmt", "phpinfo", "agreed",
            "^pg$", "arubalp", "arrs\d", "^highlight$", "imagefield\.(x|y)", "blast_rank",
            "^rid$", "log\$", "route", "rqid", "subid", "affiliate", "justshown", "^anx.+",
            "cfg_(basedir|imgtype|mediatype|not_allowall||softtype)", "^drv$", "^sz$", "^ti$",
            "^(c|C)(7|9)$", "^oe$", "^channel$", "^gfe_rd$", "^1\'$", "^o$", "^oh$", "^efg$",
            ".*\+{2,}.*", "^db\/cgi-bin\/instancebrowser\?db$", "^(json(p(-|_)?)?)?callback$", "^dir(_inc)?$" ]

        blacklist_values => [ ".*", "<\/123"]
    }

    # IP-based geolocation.
    geoip {
      source => "clientip"
    }
    # Check to see if this was a possible "page view" (exclude bots and requests coming from the same server)
    # header.php and footer.php are PHP files but should *not* count as page views (they're not really pages).
    if ![probably_a_bot] and ![internal_IP] and [request] !~ "header.php" and [request] !~ "footer.php" {
        grok {
            match => ["request", "%{NONPAGEVIEW_FILES}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_file_extension_test_parse_fail"]
            # If it matches the pattern for NONPAGEVIEW_FILES, it was probably NOT a page view.
            add_field => {"probably_not_a_page_view" => true }
        }
    }
    # Use the "aggregate" filter to detect page "visitors" (same IP address requesting "pages" >= 3 times in 60 minutes)
    # exclude IP 206.108.125.163 - it is the Reactome server!
    if ![probably_not_a_page_view] and ![internal_IP] and ![probably_a_bot] and [request] !~ "header.php" and [request] !~ "footer.php" and [clientip] != "206.108.125.163"  {
        aggregate {
            task_id => "%{clientip}"
            code => 'map["reactome_pages_viewed_count"] ||= 0
                     map["reactome_pages_viewed_count"] += 1
                     if map["reactome_pages_viewed_count"] == 1
                         map["@first_timestamp"] = event.get("@apache_timestamp")
                         # event.set("@timestamp",event.get("@apache_timestamp"))
                     elsif  map["reactome_pages_viewed_count"] > 1
                         diff = (event.get("@apache_timestamp") - map["@first_timestamp"]).to_i
                         if diff > 3600
                             map["visit_length_in_seconds"]=diff
                             event.set("is_page_visitor",true)
                             # map["@end_timestamp"] = event.get("@apache_timestamp")
                         # else
                         #     event.set("visit_too_long",true)
                         end
                     end
                     # map["reactome_pages_viewed"] ||= []
                     # map["reactome_pages_viewed"].push(event.get("request"))
                     # map["last_apache_timestamp"] = event.get("@apache_timestamp")
                     '
            # timeout_tags => ['_aggregatetimeout']
            # timeout_code => 'event.set("reactome_multiple_pages_viewed", event.get("reactome_pages_viewed_count") > 1)
            #                  event.set("@timestamp",event.get("last_apache_timestamp"))
            #                  event.set("@apache_timestamp",event.get("last_apache_timestamp"))'
        }
    }

    if [is_page_visitor] {
        aggregate {
            task_id => "%{clientip}"
            end_of_task => true
            # push_map_as_event_on_timeout => true
            # timeout_task_id_field => "clientip"
            timeout => 600 # unit is "second"
            # timeout_code => ''
            map_action => "update"
            code => 'event.set("reactome_multiple_pages_viewed", map["reactome_pages_viewed_count"] > 1)
                     # event.set("@timestamp",event.get("last_apache_timestamp"))
                     event.set("@end_timestamp", event.get("@apache_timestamp"))
                     event.set("@first_timestamp", map["@first_timestamp"])
                     event.set("reactome_pages_viewed_count", map["reactome_pages_viewed_count"])
                     event.set("visit_length_in_seconds", map["visit_length_in_seconds"])
                     # map["@apache_timestamp"] = event.get("@apache_timestamp")
                     '
        }
    }

}
output {
    if [reactome_server] == "reactomews" {
        elasticsearch {
            hosts => ["esserver:9200"]
            index => "reactomews"
            document_id => "%{[@metadata][fingerprint]}"
        }
    }
    else {
        if [!reactome_pages_viewed_count] {
            elasticsearch {
                hosts => ["esserver:9200"]
                index => "reactome-main"
                document_id => "%{[@metadata][fingerprint]}"
            }
        }
        # If there IS a "reactome_pages_viewed_count" then the event is an AGGREGATE and we can't use the metadata fingerprint, let ES generate its own ID.
        else {
            elasticsearch {
                hosts => ["esserver:9200"]
                index => "reactome-main"
            }
       }
    }
}
