input {
    beats {
        port => "5043"
    }
}
filter {
    # Parse Apache log message.
    grok {
      match => { "message" => ["%{COMBINEDAPACHELOG}"] }
      tag_on_failure => ["grok_not_APACHE_log"]
    }
    # extract date from logfile and set it as a proper "date" field, so it can be indexed and used instead the default timestamp.
    date {
      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@apache_timestamp"
    }
    # decode encoded values in the message.
    urldecode {
        field => "message"
    }
    grok {
        match => [ "message", "%{URIPARAM:url}" ]
        tag_on_failure => ["grok_no_URIPARAM"]
    }
    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        match => ["referrer", "%{REFERRER_URL:referrer_url}"]
        tag_on_failure => ["grok_referrer_parse_err"]
    }

    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        match => [ "request" , "%{STABLE_IDENTIFIER:reactome_stable_identifier}"]
        tag_on_failure => ["grok_no_reactome_stable_identifier"]
    }
    grok {
        match => ["request", "%{REACTOME_APPLICATION:reactome_application}"]
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        tag_on_failure => ["grok_no_reactome_application"]
    }
    # Add a field to indicate a request came from an internal IP address (relative to the server)
    # These requests are probably coming from other Reactome servers, OR from OICR staff.
    cidr {
        add_tag => ["internal_IP"]
        address => [ "%{clientip}" ]
        network => [ "10.0.0.0/8", "172.16.0.0/12", "192.168.0.0/16" ]
    }
    if [request] =~ "ReactomeRESTfulAPI" {
        grok {
            match => ["request", "%{RESTFUL_API_PATH}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_RESTfulAPI_path"]
        }
    }
    if [request] =~ "ContentService" {
        grok {
            match => ["request", "%{CONTENT_SERVICE_PATH}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_ContentService_path"]
        }
    }
    if [request] =~ "AnalysisService" {
        grok {
            match => ["request", "%{ANALYSIS_SERVICE_PATH}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_AnalysisService_path"]
        }
    }
    if [request] =~ "cgi-bin" {
        grok {
            match => ["request", "%{CGI_SCRIPT_NAME}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_CGIScipt"]
        }
    }
    # get the download file name
    if [reactome_application] == "download/current/" {
        grok {
            match => ["request", "%{DOWNLOAD_FILE}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_parse_fail_DownloadFileName"]
        }
    }
    # transform the bytes field to an integer so Kibana can do SUM on it. Also clean up the URL a bit.
    mutate {
        convert => {"bytes" => "integer"}
        gsub => [
                "url","&amp;","&",
                "url","&amp%3B","&",
                "url","amp%3B","&",
                "url","%3B",";",
                "url","&amp","&",
                "url","%230","",
                "url","load%5B%5D","load",
                "url",";amp;id","id"
                ]
    }
    # Extract key-value pairs from URL string and index the keys, but only if they are REACTOME keys.
    # Ignore when the referrer is a search engine.
    # Also ignore when referrer is WordPress admin or Joomla admin (we don't need to index those keys for log analysis)
    if ([referrer] =~ "http:\/\/reactome.org" or [referrer] =~ "https:\/\/reactome.org"
            or [referrer] =~ "http:\/\/www.reactome.org" or [referrer] =~ "https:\/\/www.reactome.org"
            or [referrer] =~ "http:\/\/reactomeprd1.oicr.on.ca" or [referrer] =~ "https:\/\/reactomeprd1.oicr.on.ca" )
        and ([referrer] !~ "google" and [referrer] !~ "yahoo" and [referrer] !~ "baidu" and [referrer] !~ "bing" and [referrer] !~ "yandex"
            and [referrer] !~ "ask." and [referrer] !~ "aol." and [referrer] !~ "duckduckgo" and [referrer] !~ "wolframalpha"
            and [referrer] !~ "wp-admin" and [referrer] !~ "wordpress" and [referrer] !~ "\/administrator\/index.php") {

        kv {
            source => "url"
            field_split => "&"
            trim_key => "\?"
            transform_key => "lowercase"
        }
    }
    prune {
        # a list of keys we want to prune. This list may grow over time.
        blacklist_names => [".*zoom.*",".*ZOOM.*",".*z00m.*","xml_sitemap", "toolbar", "treeView", "_fp[0-9]+","^(\'(A|a))$","_sm_au_",".*<script>.*", "width", "height","_wpnonce","<meta%20http-equiv" ]
    }
    # process the useragent information
    useragent {
      source => "agent"
      target => "useragent"
    }
    # Add a field to make it easier to find messages from bots.
    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        match => ["useragent", "%{BOT}"]
        tag_on_failure => ["grok_BOT_parse_fail"]
        add_field => {"probably_a_bot" => true }
    }

    # IP-based geolocation.
    geoip {
      source => "clientip"
  }
}
output {
  elasticsearch {
    hosts => ["esserver:9200"]
  }

}
