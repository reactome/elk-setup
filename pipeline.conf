input {
    beats {
        port => "5043"
    }
}
filter {
    # Fingerprint to avoid duplicate messages
    fingerprint {
        source => "message"
        target => "[@metadata][fingerprint]"
        method => "MURMUR3"
    }
    # Rename the "agent" field from beats because it might clash with previously existing "agent" objects
    # The new "agent" from beats is an OBJECT with details about the beats agent, the old "agent" was
    # parsed from the message and was a STRING with details about the user-agent - the same name with
    # different structures seems to confuse elastic when re-indexing old data.
    mutate {
        rename => ["agent", "beats_agent"]
    }
    # For now, we have reactomews and everything else is assumed to come from reactome.org
    if [log][file][path] =~ "cpws_tomcat" or [log][file][path] =~ "reactomews" {
        mutate {
            add_field => {"reactome_server" => "reactomews"}
        }
        # Parse Apache log message.
        grok {
          # Useragent field no longer works from COMBINEDAPACHELOG pattern.
          # For workaround used here, see: https://github.com/logstash-plugins/logstash-patterns-core/issues/243#issuecomment-537907058
          match => { "message" => ["%{HTTPD_COMMONLOG}"] }
          tag_on_failure => ["grok_message_parsing_failure"]
        }
    }
    # IDG logs get a different server value
    else if [log][file][path] =~ "idg" {
        mutate {
            add_field => {"reactome_server" => "idg.reactome.org"}
        }
        # Parse Apache log message.
      grok {
        match => { "message" => ["%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:client_agent} (?<content_length>\"[^ \"]+\")? ?(?<cookie>\"[^ \"]+\")?( %{NUMBER:bytes_received} %{NUMBER:time_to_serve_request})?"] }
        tag_on_failure => ["grok_not_COMMONAPACHELOG"]
      }
    }
    else {
        mutate {
            add_field => {"reactome_server" => "reactome.org"}
        }
        # Parse Apache log message.
        grok {
          match => { "message" => ["%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:client_agent} (?<content_length>\"[^ \"]+\")? ?(?<cookie>\"[^ \"]+\")?( %{NUMBER:bytes_received} %{NUMBER:time_to_serve_request})?"] }
          tag_on_failure => ["grok_not_COMMONAPACHELOG"]
        }
    }
    # extract date from logfile and set it as a proper "date" field, so it can be indexed and used instead the default timestamp.
    date {
      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@apache_timestamp"
    }

    # decode encoded values in the message, request, and referrer.
    # Do this AFTER parsing the message as an Apache log message.
    # Otherwise, decoded characters might break the regex.
    urldecode {
        field => "message"
        tag_on_failure => ["_urldecodefailure_message"]
    }
    urldecode {
        field => "request"
        tag_on_failure => ["_urldecodefailure_request"]
    }
    urldecode {
        field => "referrer"
        tag_on_failure => ["_urldecodefailure_referrer"]
    }

    # process the useragent information
    useragent {
      source => "client_agent"
      target => "useragent"
      lru_cache_size => 2000
    }

    # Previously, this was done with a scripted field, but I think that was too slow.
    if [response]
    {
        ruby {
            code => ' event.set("http_code_category", event.get("response")[0]+"xx" ) '
        }
    }
    # Now, extract some specific date fields, so that they can be used in reports.
    # Sometimes I see 'Ruby exception occurred: no time information in ""' in the logs.
    # I don't know how it gets there but if there's a problem with @apache_timestamp,
    # let's detect it and tag the event appropriately.
    if [@apache_timestamp]
    {
        ruby {
            code => ' event.set("dayOfWeek", Time.parse(event.get("@apache_timestamp").to_s).strftime("%w - %A"))
                      event.set("weekOfYear", Time.parse(event.get("@apache_timestamp").to_s).strftime("%U")) '
        }
    }
    else
    {
        mutate {
            add_tag => ["ERR_on_apache_timestamp"]
        }
    }
    grok {
      match => { "message" => ["^.*%{MONTHDAY:dayOfMonth}\/%{MONTH:monthName}\/%{YEAR:year}:(?!<[0-9])%{HOUR:hourOfDay}:%{MINUTE}(?::%{SECOND})(?![0-9]) %{INT:utcOffset}.*$"] }
    }
    # Extract URL parameters from the "request" field.
    grok {
        match => [ "request", "%{URIPARAM:url}" ]
        tag_on_failure => ["grok_no_URIPARAM"]
    }
    # Add a field to indicate a request came from an internal IP address (relative to the server)
    # These requests are probably coming from other Reactome servers, OR from OICR staff.
    if [clientip] {
        cidr {
            add_field => {"internal_IP" => "true" }
            address => [ "%{clientip}" ]
            network => [ "10.0.0.0/8", "172.16.0.0/12", "192.168.0.0/16", "fc00::/7", "::1", "::", "127.0.0.1", "0.0.0.0",
                         "100.25.71.177", "172.31.43.19", "172.31.0.0/12" ]
        }
        # Let's make sure we get both values for internal_IP - it's easier to group and report on true/false values
        # than true/(FIELD DOES NOT EXIST) values.
        if ![internal_IP] {
            mutate {
                add_field => { "internal_IP" => "false" }
            }
            usage_type {
                ip_ranges_file => "/ips.csv"
                start_index => 0
                end_index => 1
                usage_type_index => 9
                ip => "clientip"
            }
        }
        # Now, let's override usage_type and set it to RSV, IFF the IP address
        # is in the IP ranges for OHSU, OICR, NYU, or EBI.
        # Use whois or https://www.ultratools.com/tools/ipWhoisLookupResult or some other tool to get IP ranges for domains
        # Use https://ip2cidr.com/ (or some other online tool) to convert IP ranges to CIDR
        cidr {
            add_field => {"usage_type_override" => "EBI" }
            address => [ "%{clientip}" ]
            # EBI network
            network => [ "193.62.192.0/21" ]
        }
        if ![usage_type_override] {
            cidr {
                add_field => {"usage_type_override" => "OICR" }
                address => [ "%{clientip}" ]
                # OICR network
                network => [ "206.108.120.0/21" ]
            }
        }
        if ![usage_type_override] {
            cidr {
                add_field => {"usage_type_override" => "OHSU" }
                address => [ "%{clientip}" ]
                # OHSU network
                network => [ "35.160.0.0/13" ]
            }
        }
        if ![usage_type_override] {
            cidr {
                add_field => {"usage_type_override" => "NYU" }
                address => [ "%{clientip}" ]
                # NYU network
                network => [ "216.165.0.0/17" ]
            }
        }

        # If there was a usage_type override, copy the original usage_type to a backup
        # field, in case anyone wants to compare them later, and then set usage_type
        # to RSV
        if [usage_type_override] {
            mutate {
                add_field => { "original_usage_type" => "%{usage_type}" }
            }
            mutate {
                update => { "usage_type" => "RSV" }
            }
        }

        # IP-based geolocation.
        geoip {
          source => "clientip"
        }
        # Some internal IP addresses should be ignored.
        if [clientip] == "10.0.0.186" or [clientip] == "10.3.0.10" {
            mutate {
                add_field => {"ignore_IP" => true}
            }
        }
        else {
            mutate {
                add_field => {"ignore_IP" => false}
            }
        }
        mutate {
            convert => {"ignore_IP" => "boolean"}
        }
    }

    # Process the referrer URL
    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        match => ["referrer", "%{REFERRER_URL}"]
        tag_on_failure => ["grok_referrer_parse_err"]
    }
    # Now, let's see if we can determine the *type* of the referrer
    if [referrer_domain] {
        if [referrer_domain] =~ /(?i)^(www\.)?reactome.*/ or ([internal_IP] and [internal_IP] == "true") {
            mutate {
                add_field => {"referrer_type" => "SELF" }
            }
        }
        else
        {
            # grok the REFERRER_DETAILS pattern to determine which type of referrer this was.
            # Referrer types:
            # SocialMeida, SearchEngine, Peer (a list of domain names that have been decided as "peers"), Other (doesn't fit into these other categories).
            # There is also SELF, which means the referrer was reactome.org; and Direct, which means there was no
            # Referrer information.
            grok {
                patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
                match => ["referrer_domain", "%{REFERRER_DETAILS}"]
                tag_on_failure => ["grok_referrer_details_parse_err"]
            }
            if [referrer_social_media_domain] {
                mutate {
                    add_field => {"referrer_type" => "SocialMedia" }
                }
            }
            else if [referrer_search_engine_domain] {
                mutate {
                    add_field => {"referrer_type" => "SearchEngine" }
                }
            }
            else if [referrer_peer_domain] {
                mutate {
                    add_field => {"referrer_type" => "Peer" }
                }
            }
            # Set to "Other", if it doesn't match any existing patterns.
            else {
                mutate {
                    add_field => {"referrer_type" => "Other" }
                }
            }
        }
    }
    else {
        # If referrer_type is still not defined, set it to Direct. Probably because
        # there IS no referrer.
        if ![referrer_type] {
            # "Direct" means the user went directly to the page requested - either by typing
            # or copy/pasting it into their browser, or by clicking a bookmark.
            mutate {
                add_field => {"referrer_type" => "Direct" }
            }
        }
    }

    # Add a field to make it easier to find messages from bots.
    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
        match => ["useragent", "%{BOT}"]
        tag_on_failure => ["grok_BOT_parse_fail"]
        add_field => {"probably_a_bot" => true }
    }

    # None of the following filters should run if ignore_IP exists.
    if ![ignore_IP]
    {
        # Extract Reactome stable identifier information from the "request" field
        grok {
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            match => [ "request" , "%{STABLE_IDENTIFIER:reactome_stable_identifier}"]
            tag_on_failure => ["grok_no_reactome_stable_identifier"]
        }
        # Extract the name of the Reactome Application that was requested (could return null)
        grok {
            match => ["request", "%{REACTOME_APPLICATION:reactome_application}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_no_reactome_application"]
        }
        if [reactome_application] =~ /(?i)\/?reactomerestfulapi/ {
            grok {
                match => ["request", "%{RESTFUL_API_PATH}"]
                patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
                tag_on_failure => ["grok_parse_fail_RESTfulAPI_path"]
            }
        }
        if [reactome_application] =~ /(?i)\/?contentservice/ {
            grok {
                match => ["request", "%{CONTENT_SERVICE_PATH}"]
                patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
                tag_on_failure => ["grok_parse_fail_ContentService_path"]
            }
        }
        if [reactome_application] =~ /(?i)\/?analysisservice/ {
            grok {
                match => ["request", "%{ANALYSIS_SERVICE_PATH}"]
                patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
                tag_on_failure => ["grok_parse_fail_AnalysisService_path"]
            }
        }
        if [reactome_application] =~ /(?i)\/?cgi-bin/ {
            grok {
                match => ["request", "%{CGI_SCRIPT_NAME}"]
                patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
                tag_on_failure => ["grok_parse_fail_CGIScipt"]
            }
        }
        # Get the download file name, and also clean up the reactome_application field
        # because the filename is probably in it.
        if [reactome_application] =~ /(?i)\/?download\/current\//
            and [reactome_application] !~ /(?i)\/?download\/current\/(fireworks|diagram)/ {

            mutate {
                replace => { "reactome_application" => "/download/current/" }
            }
            grok {
                match => ["request", "%{DOWNLOAD_FILE}"]
                patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
                tag_on_failure => ["grok_parse_fail_DownloadFileName"]
            }
        }
        if [reactome_application] =~ /(?i).*caBigR3WebApp.*/ {
            grok {
                match => ["request", "%{CABIGR3WEBAPP_PATH}"]
                patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
                tag_on_failure => ["grok_parse_fail_CABIGR3WEBAPP_PATH"]
            }
        }
        if [reactome_application]
        {
            mutate
            {
                # Normalize the reactome_application field - makes it easier for reporting
                # if /ContentService and /contentservice are treated the same way
                # (because they are the same thing).
                lowercase => [reactome_application]
            }
        }
        # Extract key-value pairs from URL string and index the keys, but only if they are REACTOME keys.
        # Ignore when the referrer is a search engine.
        # Be aware that sometimes the referrer might look like "http://www.google.com/url=http://reactome.org/blah-blahblah"
        # Also ignore when referrer is WordPress admin or Joomla admin (we don't need to index those keys for log analysis)
        # Also: do not perform KV processing when request is from a bot - too much potential junk in requests from bots.
        # Also: Don't bother with KV processing if the request is just for favicon or something trivial like that.
        # How about this: ONLY bother with KV processing for known Reactome Applications? Should simplify things by a lot!
        if ![probably_a_bot] and [reactome_application] and ([request] !~ ".*<script.*" and [request] !~ ".*script>.*")
            and [request] !~ ".*css\?v.*" and [request] !~ ".*js\?v.*"
            and ([request] !~ "^\/favicon.ico$"
                and [request] !~ "^\/icons\/search\/Pathway\.gif$"
                and [request] !~ "^\/images\/spinner\.gif$"
                and [request] !~ "^\/stylesheet\.css$"
                and [request] !~ ".*getTags\?tagName=http.*"
                and [request] !~ ".*cache\.js\?.*"
                and [request] !~ ".*\?q=http.*")
            and ([referrer_type] != "SearchEngine" and [referrer_type] != "SocialMedia"
                and [referrer] !~ "wp-admin" and [referrer] !~ "wordpress" and [referrer] !~ "\/administrator\/index.php"
                and [request] !~ "wp-admin" and [request] !~ "wordpress" and [request] !~ "\/administrator\/index.php")
        {
            kv {
                source => "url"
                field_split => "&"
                trim_key => "\?"
                transform_key => "lowercase"
                include_keys => [
                    "binsize", "chebiid", "class", "cluster", "compartments", "coverage", "db",
                    "db_id", "diagramprofile", "ehld", "excludestructures", "ext", "fieldvalue",
                    "fireworksprofile", "focus_species", "format", "id", "includedisease", "interactor",
                    "interactors", "ispathway", "margin", "max", "name", "offset", "operator", "order",
                    "ordering", "organism", "page", "pagesize", "pathway-target", "pathwayinputlist",
                    "pathways", "previouspathways", "previousproteins", "previousreactions", "proteins",
                    "protocol", "pvalue", "q", "quality", "query", "resource", "rows", "search_utils_operator",
                    "searchphrase", "searchword", "sel", "show", "sortby", "source", "sourceid", "species",
                    "speciesid", "speciestaxid", "st_id", "start", "start+row", "t", "tagname", "title", "token",
                    "type", "types", "utm_medium", "utm_source", "v"
                ]
                prefix => "kvXtrct_"
            }
        }

        # prune {
        #     # a list of keys we want to prune. This list may grow over time.
        #     blacklist_names => [".*(z|Z)(o|O|0){2}(m|M).*","(kvXtrct_)?xml_sitemap",
        #         "(kvXtrct_)?toolbar", "(kvXtrct_)?treeView", "_fp[0-9]+","^(\'(A|a))$","_sm_au_",".*<script>.*",
        #         "^(kvXtrct_)?width$", "^(kvXtrct_)?height$","_wpnonce","<meta%20http-equiv","document_srl","conversation(C|c)ontext",
        #         "^(kvXtrct_)?do$", "^(kvXtrct_)?mod$", "^(kvXtrct_)?op$", "pwb_redirect", "submit(search)?", "^(kvXtrct_)?task$", "^(kvXtrct_)?view(image)?$",
        #         "^(kvXtrct_)?keyword$", "^(kvXtrct_)?dopost$", "^(kvXtrct_)?fmdo$", "^(kvXtrct_)?array$", "c@@\.12c\.c@\.\(c\)\(c\(", "inslockfile", "install_demo_name",
        #         "^(kvXtrct_)?s_lang$", "isappinstalled", "^(kvXtrct_)?coppa$", "^(kvXtrct_)?agreed$", "^(kvXtrct_)?fid$", "mod(e)?", "visitdsttime", "preview_nonce",
        #         "^(kvXtrct_)?usg$", "^(kvXtrct_)?ved$", "^(kvXtrct_)?sa$", "tbpm", "s,ql",".*sbmlretrieval\/allforms\?speciesid.*", "redlink",
        #         "nsukey","^(kvXtrct_)?ll$", "maxadcountsperinterval", "method:#_memberaccess%\.url",
        #         "infloat", "inajax", "ajaxmenu", "handlekey", "gf_page", "^(kvXtrct_)?fid$", "entryac",
        #         "cssreloader2345", "cperpage", "classic", "chvzaa", "bustype", "beacon", "adcountintervalhours",
        #         "^(kvXtrct_)?1$", "^(kvXtrct_)?4$", "^(kvXtrct_)?2$", "stichwort", "wx_fmt", "phpinfo", "agreed",
        #         "^(kvXtrct_)?pg$", "arubalp", "arrs\d", "^(kvXtrct_)?highlight$", "imagefield\.(x|y)", "blast_rank",
        #         "^(kvXtrct_)?rid$", "log\$", "route", "rqid", "subid", "affiliate", "justshown", "^anx.+",
        #         "cfg_(basedir|imgtype|mediatype|not_allowall||softtype)", "^(kvXtrct_)?drv$", "^(kvXtrct_)?sz$", "^(kvXtrct_)?ti$",
        #         "^(kvXtrct_)?(c|C)(7|9)$", "^(kvXtrct_)?oe$", "^(kvXtrct_)?channel$", "^(kvXtrct_)?gfe_rd$", "^(kvXtrct_)?1\'$", "^(kvXtrct_)?o$", "^(kvXtrct_)?oh$", "^(kvXtrct_)?efg$",
        #         ".*\+{2,}.*", "^(kvXtrct_)?db\/cgi-bin\/instancebrowser\?db$", "^(kvXtrct_)?(json(p(-|_)?)?)?callback$", "^(kvXtrct_)?dir(_inc)?$",
        #         "crazycache", "^(kvXtrct_)?cn$","fl", "^(kvXtrct_)?rev$", "kb_tick", "j24a75", "wb48617274", "x9wj", "^(kvXtrct_)?ei$", "^(kvXtrct_)?foo$",
        #         "\'\/\*\*\/\'", "-d\+allow_url_include", "132\?cmd", "^(kvXtrct_)?;.+",
        #         "fbclid", "nslookup_button", "^(kvXtrct_)?s?http:\/\/.*", "^(kvXtrct_)?ie$", "^(kvXtrct_)?iid$", "name\.\#.*",
        #         "queryb64str", "qrahfyhjkjaasc6j", "$^(kvXtrct_)puid$", "patcher\?cmd", "sla6pddys", "yyue",
        #         "^(kvXtrct_)?at[0-9]+$", ".*convert\(.*", ".*(\(|\)).*", "(kvXtrct_)?#$", ".*9,\+9,\+9", ".*9ehhdszt",
        #         ".*%.*", ".*(x|y)_(mag|offset)", ".*sp\.nextform", "^(kvXtrct_)?skin$", "(kvXtrct_)?sg$", ".*redirect:\$\{\#a.*",
        #         ".*R0BPnN9T.*", "^(kvXtrct_)?php.*", "^(kvXtrct_)?pollvars.*", ".*new(lang|page|remember|username).*", ".*amp;.*",
        #         "^kvXtrct__escaped_fragment_", "^kvXtrct_'(a|A)", "^kvXtrct_.*;.*", "^kvXtrct_#.*", "^kvXtrct__.*", "^kvXtrct_,.*",
        #         "^kvXtrct_an.*", "^kvXtrct_gt;.*", "kvXtrct_null", "^kvXtrct_page.*\/", "kvXtrct_x-nebulaxmlhttprequest",
        #         "kvXtrct__t_t_t", "kvXtrct__dc", "kvXtrct____directive", "kvXtrct_amp\:.*", "kvXtrct_anx.*", "kvXtrct_",
        #         "kvXtrct_\..*", "kvXtrct_--.*", "kvXtrct_acc(ept)?", "kvXtrct_\d+", "kvXtrct_.*\?.*", "kvXtrct_.*\/.*"
        #         ]

            # The blacklist kept getting bigger, so instead let's try a whitelist with only the fields that we seem to need.
            # These names were determined by looking at the kvXtrct_ fields that have very high ocurrences (> 1000), and then just selecting those.
            # whitelist_names => [
            #   "kvXtrct_c", "kvXtrct_chebiid", "kvXtrct_compartments", "kvXtrct_get", "kvXtrct_number","kvXtrct_focus_species", "kvXtrct_format", "kvXtrct_personid",
            #   "kvXtrct_sourceid", "kvXtrct_speciestaxid", "kvXtrct_sugexp", "kvXtrct_type", "kvXtrct_limit", "kvXtrct_name", "kvXtrct_expcolumn", "kvXtrct_searchword",
            #   "kvXtrct_binsize", "kvXtrct_cluster", "kvXtrct_d", "kvXtrct_db", "kvXtrct_db_id", "kvXtrct_diagramprofile", "kvXtrct_ehld", "kvXtrct_excludestructures",
            #   "kvXtrct_f", "kvXtrct_fireworksprofile", "kvXtrct_from_reactome", "kvXtrct_id", "kvXtrct_includedisease", "kvXtrct_interactor", "kvXtrct_interactors",
            #   "kvXtrct_margin", "kvXtrct_max", "kvXtrct_min", "kvXtrct_ml", "kvXtrct_offset", "kvXtrct_operator", "kvXtrct_order", "kvXtrct_page", "kvXtrct_pagesize",
            #   "kvXtrct_pathways", "kvXtrct_profile", "kvXtrct_pvalue", "kvXtrct_q", "kvXtrct_quality", "kvXtrct_query", "kvXtrct_resource", "kvXtrct_rows", "kvXtrct_sel",
            #   "kvXtrct_sortby", "kvXtrct_source", "kvXtrct_species", "kvXtrct_speciesid", "kvXtrct_st_id", "kvXtrct_start", "kvXtrct_start+row", "kvXtrct_t", "kvXtrct_tagname",
            #   "kvXtrct_title", "kvXtrct_token", "kvXtrct_types", "kvXtrct_v", "kvXtrct_category", "kvXtrct_coverage", "kvXtrct_organism", "kvXtrct_pathwayinputlist",
            #   "kvXtrct_showall"
            # ]
        #     blacklist_values => [ ".*", "<\/123"]
        # }

        mutate {
            gsub => [
                    "request","&lt;&#61;&gt;","<=>",
                    "request","&#61;&gt;","=>",
                    "request","&lt;&#61;","<="
                    ]
        }

        grok {
            match => ["request", "%{NONPAGEVIEW_FILES}"]
            patterns_dir => ["/usr/share/logstash/pipeline/patterns/"]
            tag_on_failure => ["grok_file_extension_test_parse_fail"]
        }
        # Check to see if this was a possible "page view" (exclude bots and requests coming from the same server)
        # header.php and footer.php are PHP files but should *not* count as page views (they're not really pages).
        # Also check that reactome_nonpage_file_extension does not exist because pageviews won't end with a "non-page" file extension (such as css, jpg, etc...)
        if ![probably_a_bot] and ![internal_IP] and ![reactome_nonpage_file_extension] and [request] !~ "header.php" and [request] !~ "footer.php" {
            mutate {
                add_field => {"probably_not_a_page_view" => false }
            }
        }
        else {
            mutate {
                add_field => {"probably_not_a_page_view" => true }
            }
        }
        # let's create a field to indicate if a log message came from Programmatic Access
        # The query in Elastic looks like this:
        # (useragent.name:/(p|P)ytho.*/  ||  useragent.name:/(J|j)ava.*/  ||  useragent.name:/(P|p)erl.*/ || useragent.name:Other || useragent.name:/.*(c|C)url.*/ || useragent.name:/.*(W|w)get.*/)  && !internal_IP:true && !referrer_type:SELF
        if ([useragent][name] =~ /(p|P)ytho.*/ or [useragent][name] =~ /(J|j)ava.*/
            or [useragent][name] =~ /(P|p)erl.*/ or [useragent][name] == "Other"
            or [useragent][name] =~ /.*(c|C)url.*/ or [useragent][name] =~ /.*(W|w)get.*/)
            and (![internal_IP] or [internal_IP] == "false" )
            and [referrer_type] != "SELF"
        {
            mutate {
                add_field => {"programmatic_access" => true}
            }
            # Should we do a DNS reverse lookup on the IP address to get a
            # better idea of who the heavy programmatic-access users are?
            # it would probably slow down the pipeline
            # https://www.elastic.co/guide/en/logstash/current/plugins-filters-dns.html#plugins-filters-dns-reverse
        }
        else {
            mutate {
                add_field => {"programmatic_access" => false}
            }
        }
    }
    # If we got this far and there is no "probably_a_bot" field, we'll create
    # one and set it to FALSE. This makes it easier to report on this field, since
    # it will exist no matter what. Of course, some of our reports may need to
    # be updated...
    if ![probably_a_bot]
    {
        mutate {
            add_field => {"probably_a_bot" => false }
        }
    }

    # transform the bytes field to an integer so Kibana can do SUM on it. Also clean up the URL a bit.
    # Also: convert certain date fields to numbers, so they can be used in histograms.
    # Also: The string "<=>" is found in some queries after "request" is URL-decoded. But... this can
    # cause the kv filter to split on the "=" and create keys and values. So we convert that back to &lt;&#61;&gt;
    # and then there is another mutate filter after kv which will revers this mutation.
    mutate {
        convert => {"bytes" => "integer"}
        convert => {"bytes_received" => "integer"}
        convert => {"dayOfMonth" => "integer"}
        convert => {"hourOfDay" => "integer"}
        convert => {"year" => "integer"}
        convert => {"weekOfYear" => "integer"}
        convert => {"internal_IP" => "boolean"}
        convert => {"programmatic_access" => "boolean"}
        convert => {"probably_a_bot" => "boolean"}
        convert => {"probably_not_a_page_view" => "boolean"}
        convert => {"time_to_serve_request" => "integer"}
        lowercase => [monthName]
        gsub => [
                "url","&amp;","&",
                "url","&amp%3B","&",
                "url","amp%3B","&",
                "url","%3B",";",
                "url","&amp","&",
                "url","%230","",
                "url","load%5B%5D","load",
                "url",";amp;id","id",
                "url","VALUE#0","VALUE",
                "url","OPERATOR#0","OPERATOR",
                "url","ATTRIBUTE#0","ATTRIBUTE",
                "request","<=>","&lt;&#61;&gt;",
                "request","=>","&#61;&gt;",
                "request","<=","&lt;&#61;"
                ]
    }
}
output {
    if [reactome_server] == "reactomews" {
        elasticsearch {
            hosts => ["esserver:9200"]
            ssl => true
            ssl_certificate_verification => true
            cacert => "/etc/elasticsearch/certs/ca/root-ca.pem"
            index => "reactomews-%{[monthName]}-%{[year]}"
            document_id => "%{[@metadata][fingerprint]}"
            user => "logstash"
            password =>
        }
    }
    else if [reactome_server] == "idg.reactome.org" {
        elasticsearch {
          hosts => ["esserver:9200"]
          ssl => true
          ssl_certificate_verification => true
          cacert => "/etc/elasticsearch/certs/ca/root-ca.pem"
          index => "reactome-idg-%{[monthName]}-%{[year]}"
          document_id => "%{[@metadata][fingerprint]}"
          user => "logstash"
          password => 
        }
    }
    else {
        elasticsearch {
            hosts => ["esserver:9200"]
            ssl => true
            ssl_certificate_verification => true
            cacert => "/etc/elasticsearch/certs/ca/root-ca.pem"
            index => "reactome-main-%{[monthName]}-%{[year]}"
            document_id => "%{[@metadata][fingerprint]}"
            user => "logstash"
            password =>
        }
    }
}
